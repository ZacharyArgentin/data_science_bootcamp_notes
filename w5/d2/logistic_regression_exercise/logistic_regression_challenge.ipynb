{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the required packages here\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Download the data and load them into Python.\n",
    "You can find the data [**here**](https://drive.google.com/file/d/0Bz9_0VdXvv9bX0MzUEhVdmpCc3c/view?usp=sharing).\n",
    "\n",
    "**Note**\n",
    "- Features and response variables are in different files.\n",
    "- Be careful about number of spaces between the values in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"Smartphone Sensor Data/train/X_train_clean.txt\", header=None, sep=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.read_csv(\"Smartphone Sensor Data/train/y_train.txt\", header=None, sep=\"\\n\")\n",
    "y = y.iloc[:,0]  # convert dataframe to series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.drop(labels=0, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>552</th>\n",
       "      <th>553</th>\n",
       "      <th>554</th>\n",
       "      <th>555</th>\n",
       "      <th>556</th>\n",
       "      <th>557</th>\n",
       "      <th>558</th>\n",
       "      <th>559</th>\n",
       "      <th>560</th>\n",
       "      <th>561</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.288585</td>\n",
       "      <td>-0.020294</td>\n",
       "      <td>-0.132905</td>\n",
       "      <td>-0.995279</td>\n",
       "      <td>-0.983111</td>\n",
       "      <td>-0.913526</td>\n",
       "      <td>-0.995112</td>\n",
       "      <td>-0.983185</td>\n",
       "      <td>-0.923527</td>\n",
       "      <td>-0.934724</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.074323</td>\n",
       "      <td>-0.298676</td>\n",
       "      <td>-0.710304</td>\n",
       "      <td>-0.112754</td>\n",
       "      <td>0.030400</td>\n",
       "      <td>-0.464761</td>\n",
       "      <td>-0.018446</td>\n",
       "      <td>-0.841247</td>\n",
       "      <td>0.179941</td>\n",
       "      <td>-0.058627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.278419</td>\n",
       "      <td>-0.016411</td>\n",
       "      <td>-0.123520</td>\n",
       "      <td>-0.998245</td>\n",
       "      <td>-0.975300</td>\n",
       "      <td>-0.960322</td>\n",
       "      <td>-0.998807</td>\n",
       "      <td>-0.974914</td>\n",
       "      <td>-0.957686</td>\n",
       "      <td>-0.943068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.158075</td>\n",
       "      <td>-0.595051</td>\n",
       "      <td>-0.861499</td>\n",
       "      <td>0.053477</td>\n",
       "      <td>-0.007435</td>\n",
       "      <td>-0.732626</td>\n",
       "      <td>0.703511</td>\n",
       "      <td>-0.844788</td>\n",
       "      <td>0.180289</td>\n",
       "      <td>-0.054317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.279653</td>\n",
       "      <td>-0.019467</td>\n",
       "      <td>-0.113462</td>\n",
       "      <td>-0.995380</td>\n",
       "      <td>-0.967187</td>\n",
       "      <td>-0.978944</td>\n",
       "      <td>-0.996520</td>\n",
       "      <td>-0.963668</td>\n",
       "      <td>-0.977469</td>\n",
       "      <td>-0.938692</td>\n",
       "      <td>...</td>\n",
       "      <td>0.414503</td>\n",
       "      <td>-0.390748</td>\n",
       "      <td>-0.760104</td>\n",
       "      <td>-0.118559</td>\n",
       "      <td>0.177899</td>\n",
       "      <td>0.100699</td>\n",
       "      <td>0.808529</td>\n",
       "      <td>-0.848933</td>\n",
       "      <td>0.180637</td>\n",
       "      <td>-0.049118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.279174</td>\n",
       "      <td>-0.026201</td>\n",
       "      <td>-0.123283</td>\n",
       "      <td>-0.996091</td>\n",
       "      <td>-0.983403</td>\n",
       "      <td>-0.990675</td>\n",
       "      <td>-0.997099</td>\n",
       "      <td>-0.982750</td>\n",
       "      <td>-0.989302</td>\n",
       "      <td>-0.938692</td>\n",
       "      <td>...</td>\n",
       "      <td>0.404573</td>\n",
       "      <td>-0.117290</td>\n",
       "      <td>-0.482845</td>\n",
       "      <td>-0.036788</td>\n",
       "      <td>-0.012892</td>\n",
       "      <td>0.640011</td>\n",
       "      <td>-0.485366</td>\n",
       "      <td>-0.848649</td>\n",
       "      <td>0.181935</td>\n",
       "      <td>-0.047663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.276629</td>\n",
       "      <td>-0.016570</td>\n",
       "      <td>-0.115362</td>\n",
       "      <td>-0.998139</td>\n",
       "      <td>-0.980817</td>\n",
       "      <td>-0.990482</td>\n",
       "      <td>-0.998321</td>\n",
       "      <td>-0.979672</td>\n",
       "      <td>-0.990441</td>\n",
       "      <td>-0.942469</td>\n",
       "      <td>...</td>\n",
       "      <td>0.087753</td>\n",
       "      <td>-0.351471</td>\n",
       "      <td>-0.699205</td>\n",
       "      <td>0.123320</td>\n",
       "      <td>0.122542</td>\n",
       "      <td>0.693578</td>\n",
       "      <td>-0.615971</td>\n",
       "      <td>-0.847865</td>\n",
       "      <td>0.185151</td>\n",
       "      <td>-0.043892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7347</th>\n",
       "      <td>0.299665</td>\n",
       "      <td>-0.057193</td>\n",
       "      <td>-0.181233</td>\n",
       "      <td>-0.195387</td>\n",
       "      <td>0.039905</td>\n",
       "      <td>0.077078</td>\n",
       "      <td>-0.282301</td>\n",
       "      <td>0.043616</td>\n",
       "      <td>0.060410</td>\n",
       "      <td>0.210795</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.070157</td>\n",
       "      <td>-0.588433</td>\n",
       "      <td>-0.880324</td>\n",
       "      <td>-0.190437</td>\n",
       "      <td>0.829718</td>\n",
       "      <td>0.206972</td>\n",
       "      <td>-0.425619</td>\n",
       "      <td>-0.791883</td>\n",
       "      <td>0.238604</td>\n",
       "      <td>0.049819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7348</th>\n",
       "      <td>0.273853</td>\n",
       "      <td>-0.007749</td>\n",
       "      <td>-0.147468</td>\n",
       "      <td>-0.235309</td>\n",
       "      <td>0.004816</td>\n",
       "      <td>0.059280</td>\n",
       "      <td>-0.322552</td>\n",
       "      <td>-0.029456</td>\n",
       "      <td>0.080585</td>\n",
       "      <td>0.117440</td>\n",
       "      <td>...</td>\n",
       "      <td>0.165259</td>\n",
       "      <td>-0.390738</td>\n",
       "      <td>-0.680744</td>\n",
       "      <td>0.064907</td>\n",
       "      <td>0.875679</td>\n",
       "      <td>-0.879033</td>\n",
       "      <td>0.400219</td>\n",
       "      <td>-0.771840</td>\n",
       "      <td>0.252676</td>\n",
       "      <td>0.050053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7349</th>\n",
       "      <td>0.273387</td>\n",
       "      <td>-0.017011</td>\n",
       "      <td>-0.045022</td>\n",
       "      <td>-0.218218</td>\n",
       "      <td>-0.103822</td>\n",
       "      <td>0.274533</td>\n",
       "      <td>-0.304515</td>\n",
       "      <td>-0.098913</td>\n",
       "      <td>0.332584</td>\n",
       "      <td>0.043999</td>\n",
       "      <td>...</td>\n",
       "      <td>0.195034</td>\n",
       "      <td>0.025145</td>\n",
       "      <td>-0.304029</td>\n",
       "      <td>0.052806</td>\n",
       "      <td>-0.266724</td>\n",
       "      <td>0.864404</td>\n",
       "      <td>0.701169</td>\n",
       "      <td>-0.779133</td>\n",
       "      <td>0.249145</td>\n",
       "      <td>0.040811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7350</th>\n",
       "      <td>0.289654</td>\n",
       "      <td>-0.018843</td>\n",
       "      <td>-0.158281</td>\n",
       "      <td>-0.219139</td>\n",
       "      <td>-0.111412</td>\n",
       "      <td>0.268893</td>\n",
       "      <td>-0.310487</td>\n",
       "      <td>-0.068200</td>\n",
       "      <td>0.319473</td>\n",
       "      <td>0.101702</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013865</td>\n",
       "      <td>0.063907</td>\n",
       "      <td>-0.344314</td>\n",
       "      <td>-0.101360</td>\n",
       "      <td>0.700740</td>\n",
       "      <td>0.936674</td>\n",
       "      <td>-0.589479</td>\n",
       "      <td>-0.785181</td>\n",
       "      <td>0.246432</td>\n",
       "      <td>0.025339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7351</th>\n",
       "      <td>0.351503</td>\n",
       "      <td>-0.012423</td>\n",
       "      <td>-0.203867</td>\n",
       "      <td>-0.269270</td>\n",
       "      <td>-0.087212</td>\n",
       "      <td>0.177404</td>\n",
       "      <td>-0.377404</td>\n",
       "      <td>-0.038678</td>\n",
       "      <td>0.229430</td>\n",
       "      <td>0.269013</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.058402</td>\n",
       "      <td>-0.387052</td>\n",
       "      <td>-0.740738</td>\n",
       "      <td>-0.280088</td>\n",
       "      <td>-0.007739</td>\n",
       "      <td>-0.056088</td>\n",
       "      <td>-0.616956</td>\n",
       "      <td>-0.783267</td>\n",
       "      <td>0.246809</td>\n",
       "      <td>0.036695</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7352 rows × 561 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           1         2         3         4         5         6         7    \\\n",
       "0     0.288585 -0.020294 -0.132905 -0.995279 -0.983111 -0.913526 -0.995112   \n",
       "1     0.278419 -0.016411 -0.123520 -0.998245 -0.975300 -0.960322 -0.998807   \n",
       "2     0.279653 -0.019467 -0.113462 -0.995380 -0.967187 -0.978944 -0.996520   \n",
       "3     0.279174 -0.026201 -0.123283 -0.996091 -0.983403 -0.990675 -0.997099   \n",
       "4     0.276629 -0.016570 -0.115362 -0.998139 -0.980817 -0.990482 -0.998321   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "7347  0.299665 -0.057193 -0.181233 -0.195387  0.039905  0.077078 -0.282301   \n",
       "7348  0.273853 -0.007749 -0.147468 -0.235309  0.004816  0.059280 -0.322552   \n",
       "7349  0.273387 -0.017011 -0.045022 -0.218218 -0.103822  0.274533 -0.304515   \n",
       "7350  0.289654 -0.018843 -0.158281 -0.219139 -0.111412  0.268893 -0.310487   \n",
       "7351  0.351503 -0.012423 -0.203867 -0.269270 -0.087212  0.177404 -0.377404   \n",
       "\n",
       "           8         9         10   ...       552       553       554  \\\n",
       "0    -0.983185 -0.923527 -0.934724  ... -0.074323 -0.298676 -0.710304   \n",
       "1    -0.974914 -0.957686 -0.943068  ...  0.158075 -0.595051 -0.861499   \n",
       "2    -0.963668 -0.977469 -0.938692  ...  0.414503 -0.390748 -0.760104   \n",
       "3    -0.982750 -0.989302 -0.938692  ...  0.404573 -0.117290 -0.482845   \n",
       "4    -0.979672 -0.990441 -0.942469  ...  0.087753 -0.351471 -0.699205   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "7347  0.043616  0.060410  0.210795  ... -0.070157 -0.588433 -0.880324   \n",
       "7348 -0.029456  0.080585  0.117440  ...  0.165259 -0.390738 -0.680744   \n",
       "7349 -0.098913  0.332584  0.043999  ...  0.195034  0.025145 -0.304029   \n",
       "7350 -0.068200  0.319473  0.101702  ...  0.013865  0.063907 -0.344314   \n",
       "7351 -0.038678  0.229430  0.269013  ... -0.058402 -0.387052 -0.740738   \n",
       "\n",
       "           555       556       557       558       559       560       561  \n",
       "0    -0.112754  0.030400 -0.464761 -0.018446 -0.841247  0.179941 -0.058627  \n",
       "1     0.053477 -0.007435 -0.732626  0.703511 -0.844788  0.180289 -0.054317  \n",
       "2    -0.118559  0.177899  0.100699  0.808529 -0.848933  0.180637 -0.049118  \n",
       "3    -0.036788 -0.012892  0.640011 -0.485366 -0.848649  0.181935 -0.047663  \n",
       "4     0.123320  0.122542  0.693578 -0.615971 -0.847865  0.185151 -0.043892  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "7347 -0.190437  0.829718  0.206972 -0.425619 -0.791883  0.238604  0.049819  \n",
       "7348  0.064907  0.875679 -0.879033  0.400219 -0.771840  0.252676  0.050053  \n",
       "7349  0.052806 -0.266724  0.864404  0.701169 -0.779133  0.249145  0.040811  \n",
       "7350 -0.101360  0.700740  0.936674 -0.589479 -0.785181  0.246432  0.025339  \n",
       "7351 -0.280088 -0.007739 -0.056088 -0.616956 -0.783267  0.246809  0.036695  \n",
       "\n",
       "[7352 rows x 561 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       5\n",
       "1       5\n",
       "2       5\n",
       "3       5\n",
       "4       5\n",
       "       ..\n",
       "7347    2\n",
       "7348    2\n",
       "7349    2\n",
       "7350    2\n",
       "7351    2\n",
       "Name: 0, Length: 7352, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Create a binary target variable: categories 1,2,3 --> 1, categories 4,5,6 --> 0 \n",
    "This will represent a binary variable indicating if person is walking or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_binary(y):\n",
    "    return np.array([0 if num in [1,2,3] else 1 for num in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_bin = make_binary(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1 is walking 0 is not walking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "\n",
    "+ Create a Univariate Binary Logistic Regression with feature number 54, which represents `tGravityAcc-min()-Y`: gravity acceleration signals in direction of Y.\n",
    "+ Compare the results of the Logistic regressions from different Python packages (sklearn, statsmodel).\n",
    "+ Plot the **fit** of predicted probabilities to the original values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = X.loc[:,54]\n",
    "X1 = X1.to_numpy().reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y_bin, train_size=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model1 = LogisticRegression()\n",
    "model1.fit(X1_train, y1_train)\n",
    "pred1 = model1.predict(X1_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7579877634262406"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.score(X1_test, y1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[532, 116],\n",
       "       [240, 583]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y1_test, pred1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### plot relationship between feature 54 and walk probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "walk_proba = model1.predict_proba(X1_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({\"tGravAccY\": X1_test.reshape(-1), \"Walk Probability\": walk_proba})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.sort_values(\"tGravAccY\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(12,8))\n",
    "plt.plot(df1[\"tGravAccY\"], df1[\"Walk Probability\"])\n",
    "\n",
    "plt.xlabel(\"tGravAccY\")\n",
    "plt.ylabel(\"Walk Probability\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4\n",
    "- Try to fit a Binary Logistic Regression with all the features? How many are significant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X4_train, X4_test, y4_train, y4_test = train_test_split(X, y_bin)\n",
    "model4 = LogisticRegression()\n",
    "model4.fit(X4_train, y4_train)\n",
    "pred4 = model4.predict(X4_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4.score(X4_test, y4_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 829,    0],\n",
       "       [   0, 1009]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y4_test, pred4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> wow, perfect result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### testing feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 561)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4.coef_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = model4.coef_.flatten()\n",
    "coef_df = pd.DataFrame(enumerate(coefs, start=1), columns=[\"Feature\", \"Coef\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_df = abs(coef_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# top 40 weighted features\n",
    "top_40 = coef_df.sort_values(\"Coef\", ascending=False).head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 64,  10, 367, 103,  57, 288, 515, 160,  53, 514, 235, 105, 370,\n",
       "       272, 269,  51, 368,  41, 104,   4,  23,  42, 141, 458,   7, 183,\n",
       "       457, 185, 159, 275, 511, 369, 266, 289,  58, 559, 294, 290,  93,\n",
       "       524])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_40.Feature.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiplying the coeficients by their standard deviation to get a better feel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale coeficients\n",
    "top_feats_scaled = pd.DataFrame(np.std(X)*np.abs(model4.coef_.flatten())).sort_values(0, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([367, 288,  10,  57, 103, 235, 368, 105, 104,  53,  64, 511, 289,\n",
       "       369,  79, 524, 185, 160,  41, 183, 290, 272, 269,  23,   4, 559,\n",
       "       261, 515,   7, 514, 209, 222,  51, 275, 266,  58,  50, 448, 370,\n",
       "        42])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_feats_scaled.head(40).index.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5\n",
    "Now, let's fit Multinomial Logistic regression to predict all categories. Firstly, we can start with **Univariate** model for these features number separately:\n",
    "+ 4\n",
    "+ 54\n",
    "- 19\n",
    "\n",
    "Check the contingency matrix to see the effect of particular features!! (each feature can be good in predicting different categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import contingency_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting only fetures 4, 19, and 54\n",
    "X5 = X[[4,19,54]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X5_train, X5_test, y5_train, y5_test = train_test_split(X5, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model using only feature 4\n",
    "model5 = LogisticRegression()\n",
    "model5.fit(X5_train[4].to_numpy().reshape(-1,1), y5_train)\n",
    "pred5 = model5.predict(X5_test[4].to_numpy().reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using only feature 19\n",
    "model7 = LogisticRegression()\n",
    "model7.fit(X5_train[19].to_numpy().reshape(-1,1), y5_train)\n",
    "pred7 = model7.predict(X5_test[19].to_numpy().reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using only feature 54\n",
    "model6 = LogisticRegression()\n",
    "model6.fit(X5_train[54].to_numpy().reshape(-1,1), y5_train)\n",
    "pred6 = model6.predict(X5_test[54].to_numpy().reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5261153427638737"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model5.score(X5_test[4].to_numpy().reshape(-1,1), y5_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 1],\n",
       "       ...,\n",
       "       [0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contingency_matrix(X5_test[4].to_numpy().reshape(-1,1), y5_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1485310119695321"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model6.score(X5_test[19].to_numpy().reshape(-1,1), y5_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1],\n",
       "       ...,\n",
       "       [0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contingency_matrix(X5_test[19].to_numpy().reshape(-1,1), y5_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.60      0.85      0.70       308\n",
      "           2       0.54      0.21      0.30       281\n",
      "           3       0.71      0.85      0.77       237\n",
      "           4       0.00      0.00      0.00       332\n",
      "           5       0.41      0.76      0.54       355\n",
      "           6       0.49      0.54      0.51       325\n",
      "\n",
      "    accuracy                           0.53      1838\n",
      "   macro avg       0.46      0.53      0.47      1838\n",
      "weighted avg       0.44      0.53      0.46      1838\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zacharyargentin/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/zacharyargentin/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/zacharyargentin/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y5_test, pred5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10772578890097932"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model5.score(X5_test[54].to_numpy().reshape(-1,1), y5_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 1, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contingency_matrix(X5_test[54].to_numpy().reshape(-1,1), y5_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6\n",
    "\n",
    "Fit the Multinomial Logistic Regression model again. Now, try to choose **all** the important features we have in the dataset. Compare with your peers on who will get the best predictions with the smallest number of features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X8 = X[top_40.Feature.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X8_train, X8_test, y8_train, y8_test = train_test_split(X8, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model8 = LogisticRegression(max_iter=1000)\n",
    "model8.fit(X8_train, y8_train)\n",
    "pred8 = model8.predict(X8_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9499455930359086"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model8.score(X8_test, y8_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[304,   9,   2,   0,   0,   0],\n",
       "       [ 10, 263,   4,   0,   0,   0],\n",
       "       [  2,   6, 213,   0,   0,   0],\n",
       "       [  0,   0,   0, 287,  33,   1],\n",
       "       [  0,   0,   0,  23, 315,   0],\n",
       "       [  0,   2,   0,   0,   0, 364]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y8_test, pred8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.96      0.97      0.96       315\n",
      "           2       0.94      0.95      0.94       277\n",
      "           3       0.97      0.96      0.97       221\n",
      "           4       0.93      0.89      0.91       321\n",
      "           5       0.91      0.93      0.92       338\n",
      "           6       1.00      0.99      1.00       366\n",
      "\n",
      "    accuracy                           0.95      1838\n",
      "   macro avg       0.95      0.95      0.95      1838\n",
      "weighted avg       0.95      0.95      0.95      1838\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y8_test, pred8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "X9 = X[top_feats_scaled.index.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "X9_train, X9_test, y9_train, y9_test = train_test_split(X9, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "model9 = LogisticRegression(max_iter=2000)\n",
    "model9.fit(X9_train, y9_train)\n",
    "pred9 = model9.predict(X9_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9869423286180631"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model9.score(X9_test, y9_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[309,   0,   0,   0,   0,   0],\n",
       "       [  0, 280,   0,   0,   0,   0],\n",
       "       [  1,   1, 252,   0,   0,   0],\n",
       "       [  0,   0,   0, 299,  15,   0],\n",
       "       [  0,   0,   0,   7, 323,   0],\n",
       "       [  0,   0,   0,   0,   0, 351]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y9_test, pred9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00       309\n",
      "           2       1.00      1.00      1.00       280\n",
      "           3       1.00      0.99      1.00       254\n",
      "           4       0.98      0.95      0.96       314\n",
      "           5       0.96      0.98      0.97       330\n",
      "           6       1.00      1.00      1.00       351\n",
      "\n",
      "    accuracy                           0.99      1838\n",
      "   macro avg       0.99      0.99      0.99      1838\n",
      "weighted avg       0.99      0.99      0.99      1838\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y9_test, pred9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Looking at the weights of each feature scaled by the standard deviation of that feature gave a great look at which features were most important and allowed to reduce dimensionality by only selecting the best features and the result was pretty good! Better than looking at the coeficients without scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7 (Stretch)\n",
    "Create your own function for Stepwise selection. Use either sklearn or statsmodel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
