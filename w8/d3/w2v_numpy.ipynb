{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0322c79b-93fa-41ec-9731-3b85a6a1acc4",
   "metadata": {},
   "source": [
    "# Word2Vec with Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6a07a01-a0c6-4c52-938a-e1011791421b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "320265ed-d1fd-4c40-b96f-ab20785de747",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-13-d459693fdef9>, line 36)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-d459693fdef9>\"\u001b[0;36m, line \u001b[0;32m36\u001b[0m\n\u001b[0;31m    training_data.append([w_target, w_context])\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class word2vec():\n",
    "    def __init__(self):\n",
    "        self.n = settings[\"n\"]\n",
    "        self.lr = settings[\"learning_rate\"]\n",
    "        self.epochs = settings[\"epochs\"]\n",
    "        self.window = settings[\"window_size\"]\n",
    "        \n",
    "        \n",
    "    def generate_training_data(self, settings, corpus):\n",
    "        # find unique word counts\n",
    "        word_counts = defaultdict(int)\n",
    "        for row in corpus:\n",
    "            for word in row:\n",
    "                word_counts[word] += 1\n",
    "        self.v_count = len(word_counts.keys())  # num of words in vocabulary\n",
    "        self.words_list = list(word_counts.keys()) # list of words in vocabulary\n",
    "        self.word_index = dict((word, i) for i, word in enumerate(self.words_list))\n",
    "        self.index_word = dict((i, word) for i, word in enumerate(self.words_list))\n",
    "        \n",
    "        training_data = []\n",
    "        for sentence in corpus:\n",
    "            sent_len = len(sentence)\n",
    "            for i, word in enumerate(sentence):\n",
    "                # Convert target word to one-hot\n",
    "                w_target = self.word2onehot(sentence[i])\n",
    "                # cycle thru context window\n",
    "                w_context = []\n",
    "                for j in range(i - self.window, i + self.window + 1):\n",
    "                    # Criteria for context word \n",
    "                    # 1. Target word cannot be context word (j != i)\n",
    "                    # 2. Index must be greater or equal than 0 (j >= 0) - if not list index out of range\n",
    "                    # 3. Index must be less or equal than length of sentence (j <= sent_len-1) - if not list index out of range \n",
    "                    if j != i and j <= sent_len-1 and j >= 0:\n",
    "                        w_context.append(self.word2onehot(sentence[j])\n",
    "                        # training_data contains a one-hot representation of the target word and context words\n",
    "            training_data.append([w_target, w_context])\n",
    "        return np.array(training_data)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf10197e-c720-4050-89bb-dbf441f1d132",
   "metadata": {},
   "outputs": [],
   "source": [
    "class word2vec():\n",
    "  def __init__(self):\n",
    "    self.n = settings['n']\n",
    "    self.lr = settings['learning_rate']\n",
    "    self.epochs = settings['epochs']\n",
    "    self.window = settings['window_size']\n",
    "\n",
    "  def generate_training_data(self, settings, corpus):\n",
    "    # Find unique word counts using dictonary\n",
    "    word_counts = defaultdict(int)\n",
    "    for row in corpus:\n",
    "      for word in row:\n",
    "        word_counts[word] += 1\n",
    "    ## How many unique words in vocab? 9\n",
    "    self.v_count = len(word_counts.keys())\n",
    "    # Generate Lookup Dictionaries (vocab)\n",
    "    self.words_list = list(word_counts.keys())\n",
    "    # Generate word:index\n",
    "    self.word_index = dict((word, i) for i, word in enumerate(self.words_list))\n",
    "    # Generate index:word\n",
    "    self.index_word = dict((i, word) for i, word in enumerate(self.words_list))\n",
    "\n",
    "    training_data = []\n",
    "    # Cycle through each sentence in corpus\n",
    "    for sentence in corpus:\n",
    "      sent_len = len(sentence)\n",
    "      # Cycle through each word in sentence\n",
    "      for i, word in enumerate(sentence):\n",
    "        # Convert target word to one-hot\n",
    "        w_target = self.word2onehot(sentence[i])\n",
    "        # Cycle through context window\n",
    "        w_context = []\n",
    "        # Note: window_size 2 will have range of 5 values\n",
    "        for j in range(i - self.window, i + self.window+1):\n",
    "          # Criteria for context word \n",
    "          # 1. Target word cannot be context word (j != i)\n",
    "          # 2. Index must be greater or equal than 0 (j >= 0) - if not list index out of range\n",
    "          # 3. Index must be less or equal than length of sentence (j <= sent_len-1) - if not list index out of range \n",
    "          if j != i and j <= sent_len-1 and j >= 0:\n",
    "            # Append the one-hot representation of word to w_context\n",
    "            w_context.append(self.word2onehot(sentence[j]))\n",
    "            # print(sentence[i], sentence[j]) \n",
    "            # training_data contains a one-hot representation of the target word and context words\n",
    "        training_data.append([w_target, w_context])\n",
    "    return np.array(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7947e0-67c6-4cf5-ba59-0392d0cc4c19",
   "metadata": {},
   "source": [
    "#### Generating corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ceab879-a619-4ac5-b7b5-29cf6f3bfa2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['natural',\n",
       "  'language',\n",
       "  'processing',\n",
       "  'and',\n",
       "  'machine',\n",
       "  'learning',\n",
       "  'is',\n",
       "  'fun',\n",
       "  'and',\n",
       "  'exciting']]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"natural language processing and machine learning is fun and exciting\"\n",
    "corpus = [[word.lower() for word in text.split()]]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638cfe7f-de29-4c1c-8b16-bd900927117d",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bfa5459-1e87-4263-a473-4e41154fb6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {\n",
    "'window_size': 2,      # context window +- center word\n",
    "'n': 10,               # dimensions of word embeddings, also refer to size of hidden layer\n",
    "'epochs': 50,          # number of training epochs\n",
    "'learning_rate': 0.01  # learning rate\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe50394-8291-4301-8063-373b45317f64",
   "metadata": {},
   "source": [
    "#### generate training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18255cf0-57b6-454a-b7c7-b6263578cda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = word2vec()\n",
    "training_data = w2v.generate_training_data(settings, corpus) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bootcamp",
   "language": "python",
   "name": "bootcamp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
